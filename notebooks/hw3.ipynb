{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашее задание3. Получить выше 0.92 на TEST FASHION MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Владимир Никифоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchvision as tv\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "ep: 0, train_loss: 0.47595429369743836, train_acc: 0.8388333333333333, test_loss: 0.3866492725908756, test_acc: 0.8616\n",
      "ep: 1, train_loss: 0.32928891321446035, train_acc: 0.8793, test_loss: 0.34114719498902557, test_acc: 0.8735\n",
      "ep: 2, train_loss: 0.2878545328657678, train_acc: 0.8941, test_loss: 0.35951814278960226, test_acc: 0.8769\n",
      "ep: 3, train_loss: 0.25812704303163164, train_acc: 0.90455, test_loss: 0.32843204848468305, test_acc: 0.8815\n",
      "ep: 4, train_loss: 0.23568312677931277, train_acc: 0.91165, test_loss: 0.3389575880020857, test_acc: 0.8805\n",
      "ep: 5, train_loss: 0.22140896593002563, train_acc: 0.9171833333333334, test_loss: 0.3220423936843872, test_acc: 0.8852\n",
      "ep: 6, train_loss: 0.20663493155164922, train_acc: 0.9215166666666667, test_loss: 0.3243516024202108, test_acc: 0.885\n",
      "ep: 7, train_loss: 0.19094893035102398, train_acc: 0.9281, test_loss: 0.33289343416690825, test_acc: 0.8855\n",
      "ep: 8, train_loss: 0.17933050964740996, train_acc: 0.9328333333333333, test_loss: 0.3212840100750327, test_acc: 0.8891\n",
      "ep: 9, train_loss: 0.16678721403821986, train_acc: 0.9376166666666667, test_loss: 0.32741873934864996, test_acc: 0.8886\n",
      "ep: 10, train_loss: 0.15603648934592593, train_acc: 0.9405833333333333, test_loss: 0.3419435787945986, test_acc: 0.8891\n",
      "ep: 11, train_loss: 0.1452578934424735, train_acc: 0.94615, test_loss: 0.3425229012966156, test_acc: 0.8912\n",
      "ep: 12, train_loss: 0.13628819439005344, train_acc: 0.9492666666666667, test_loss: 0.3626141220331192, test_acc: 0.8881\n",
      "ep: 13, train_loss: 0.12616941928863526, train_acc: 0.9535, test_loss: 0.3684361450374126, test_acc: 0.8916\n",
      "ep: 14, train_loss: 0.12019082469509003, train_acc: 0.9552666666666667, test_loss: 0.37674752026796343, test_acc: 0.89\n",
      "ep: 15, train_loss: 0.11021003116001474, train_acc: 0.9597666666666667, test_loss: 0.38821638450026513, test_acc: 0.8891\n",
      "ep: 16, train_loss: 0.10611365454945158, train_acc: 0.9602833333333334, test_loss: 0.43078010268509387, test_acc: 0.884\n",
      "ep: 17, train_loss: 0.10049765519005187, train_acc: 0.9626, test_loss: 0.4086843341588974, test_acc: 0.8888\n",
      "ep: 18, train_loss: 0.09534091727530702, train_acc: 0.96405, test_loss: 0.40331522598862646, test_acc: 0.8868\n",
      "ep: 19, train_loss: 0.08893329529686177, train_acc: 0.9671, test_loss: 0.40955171212553976, test_acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    #torch.nn.Dropout(p=0.15),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.BatchNorm1d(784),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(256),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    #torch.nn.Dropout(p=0.3),\n",
    "    torch.nn.BatchNorm1d(64),\n",
    "    torch.nn.Linear(64, 10)\n",
    ")\n",
    "print(model)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже на 11 эпохе достигнуто значение 0.8912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.07116884039120472, train_acc: 0.9745333333333334, test_loss: 0.3879988767206669, test_acc: 0.8957\n",
      "ep: 1, train_loss: 0.0591926505273961, train_acc: 0.98025, test_loss: 0.39003591164946555, test_acc: 0.8946\n",
      "ep: 2, train_loss: 0.0555309598829518, train_acc: 0.9820833333333333, test_loss: 0.3981344260275364, test_acc: 0.8954\n",
      "ep: 3, train_loss: 0.05322467549208631, train_acc: 0.9825833333333334, test_loss: 0.39778875187039375, test_acc: 0.894\n",
      "ep: 4, train_loss: 0.05143077236382251, train_acc: 0.9838, test_loss: 0.3915284566581249, test_acc: 0.8949\n",
      "ep: 5, train_loss: 0.04893952117479862, train_acc: 0.98485, test_loss: 0.4073649846017361, test_acc: 0.8965\n",
      "ep: 6, train_loss: 0.04812022005306914, train_acc: 0.9852666666666666, test_loss: 0.42445593886077404, test_acc: 0.8966\n",
      "ep: 7, train_loss: 0.046474754081127494, train_acc: 0.9859333333333333, test_loss: 0.38879416743293405, test_acc: 0.8958\n",
      "ep: 8, train_loss: 0.04576429064445039, train_acc: 0.9860833333333333, test_loss: 0.42344827130436896, test_acc: 0.8969\n",
      "ep: 9, train_loss: 0.04517621690447026, train_acc: 0.98625, test_loss: 0.39546974673867225, test_acc: 0.8969\n",
      "ep: 10, train_loss: 0.04370550200184609, train_acc: 0.9873, test_loss: 0.4094645731151104, test_acc: 0.8962\n",
      "ep: 11, train_loss: 0.04376206959974258, train_acc: 0.9869833333333333, test_loss: 0.3909021018072963, test_acc: 0.8959\n",
      "ep: 12, train_loss: 0.04260683685065584, train_acc: 0.9869, test_loss: 0.4123356480151415, test_acc: 0.8967\n",
      "ep: 13, train_loss: 0.042602297544796414, train_acc: 0.9874666666666667, test_loss: 0.4107929177582264, test_acc: 0.8966\n",
      "ep: 14, train_loss: 0.0413203612564409, train_acc: 0.9877333333333334, test_loss: 0.39274069517850874, test_acc: 0.8973\n",
      "ep: 15, train_loss: 0.04135612340921417, train_acc: 0.9876666666666667, test_loss: 0.4094501044601202, test_acc: 0.8966\n",
      "ep: 16, train_loss: 0.04039489657003829, train_acc: 0.9878333333333333, test_loss: 0.4035427153110504, test_acc: 0.896\n",
      "ep: 17, train_loss: 0.03955909062097682, train_acc: 0.9882666666666666, test_loss: 0.4063965087756515, test_acc: 0.8956\n",
      "ep: 18, train_loss: 0.03932698290160996, train_acc: 0.9883166666666666, test_loss: 0.39861523546278477, test_acc: 0.8955\n",
      "ep: 19, train_loss: 0.03875157607204102, train_acc: 0.9887, test_loss: 0.40905448943376543, test_acc: 0.8968\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Меняя оптимизатор получилось достичь линейными слоями 0.8973, хотя получалось и 0.9 таким же запуском."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попытка сделать автоматический поиск сходимости через движение \"тройками\" с изменением оптимизатора и learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 0, train_loss: 0.486048639708377, train_acc: 0.8356666666666667, test_loss: 0.39857340678572656, test_acc: 0.8602\n",
      "[0, 0, 0] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 1, train_loss: 0.3321280453433382, train_acc: 0.8798333333333334, test_loss: 0.36325082890689375, test_acc: 0.8689\n",
      "[0, 0, 0.8602] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 2, train_loss: 0.29080036840540296, train_acc: 0.8915333333333333, test_loss: 0.351270055025816, test_acc: 0.8768\n",
      "[0, 0.8602, 0.8689] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 3, train_loss: 0.26422193640090047, train_acc: 0.9027, test_loss: 0.33279183842241766, test_acc: 0.8787\n",
      "[0.8602, 0.8689, 0.8768] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 4, train_loss: 0.23946071584173972, train_acc: 0.9100166666666667, test_loss: 0.34136408381164074, test_acc: 0.881\n",
      "[0.8689, 0.8768, 0.8787] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 5, train_loss: 0.22470817229849227, train_acc: 0.9156, test_loss: 0.3248066153377295, test_acc: 0.8863\n",
      "[0.8768, 0.8787, 0.881] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 6, train_loss: 0.20465528127994942, train_acc: 0.9244666666666667, test_loss: 0.34413967579603194, test_acc: 0.8823\n",
      "[0.8787, 0.881, 0.8863] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 7, train_loss: 0.1948086492875789, train_acc: 0.9272666666666667, test_loss: 0.31939192600548266, test_acc: 0.8873\n",
      "[0.881, 0.8863, 0.8823] 0\n",
      "trainer_name: adam, glob_circle: 0, lr: 0.001, ep: 8, train_loss: 0.1795446846079319, train_acc: 0.933, test_loss: 0.3296043757349253, test_acc: 0.8849\n",
      "[0.8863, 0.8823, 0.8873] 0\n",
      "trainer_name: sgd, glob_circle: 0, lr: 0.001, ep: 9, train_loss: 0.15332813988974753, train_acc: 0.9431, test_loss: 0.31490029245615003, test_acc: 0.8895\n",
      "[0.8863, 0.8823, 0.8873] 1\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 0, train_loss: 0.14585351652287423, train_acc: 0.9478, test_loss: 0.3192124605178833, test_acc: 0.89\n",
      "[0.8823, 0.8873, 0.8895] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 1, train_loss: 0.1425999513014834, train_acc: 0.9491166666666667, test_loss: 0.32403663024306295, test_acc: 0.8912\n",
      "[0.8873, 0.8895, 0.89] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 2, train_loss: 0.13969495375739768, train_acc: 0.9498666666666666, test_loss: 0.3197719894349575, test_acc: 0.8917\n",
      "[0.8895, 0.89, 0.8912] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 3, train_loss: 0.13777699036167024, train_acc: 0.95075, test_loss: 0.31536253951489923, test_acc: 0.8932\n",
      "[0.89, 0.8912, 0.8917] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 4, train_loss: 0.13605719782570574, train_acc: 0.9512333333333334, test_loss: 0.3096372071653605, test_acc: 0.8937\n",
      "[0.8912, 0.8917, 0.8932] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 5, train_loss: 0.1360607032763197, train_acc: 0.9513, test_loss: 0.32643220014870167, test_acc: 0.8933\n",
      "[0.8917, 0.8932, 0.8937] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 6, train_loss: 0.13455684048698305, train_acc: 0.9520666666666666, test_loss: 0.3184010062366724, test_acc: 0.8934\n",
      "[0.8932, 0.8937, 0.8933] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 7, train_loss: 0.13350583432202642, train_acc: 0.9525833333333333, test_loss: 0.3002016685903072, test_acc: 0.8952\n",
      "[0.8937, 0.8933, 0.8934] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 8, train_loss: 0.13185372644282403, train_acc: 0.9534666666666667, test_loss: 0.3135763887315989, test_acc: 0.8942\n",
      "[0.8933, 0.8934, 0.8952] 0\n",
      "trainer_name: sgd, glob_circle: 1, lr: 0.001, ep: 9, train_loss: 0.1320698177243801, train_acc: 0.9528833333333333, test_loss: 0.3116393215954304, test_acc: 0.8922\n",
      "[0.8934, 0.8952, 0.8942] 0\n",
      "trainer_name: adam, glob_circle: 2, lr: 0.001, ep: 0, train_loss: 0.1721892162523371, train_acc: 0.9352166666666667, test_loss: 0.3412863716483116, test_acc: 0.8858\n",
      "[0.8934, 0.8952, 0.8942] 1\n",
      "trainer_name: sgd, glob_circle: 2, lr: 0.001, ep: 1, train_loss: 0.1512295198884416, train_acc: 0.9437833333333333, test_loss: 0.32752695195376874, test_acc: 0.8885\n",
      "[0.8934, 0.8952, 0.8942] 2\n",
      "trainer_name: adam, glob_circle: 2, lr: 0.001, ep: 2, train_loss: 0.15963848518564347, train_acc: 0.9396166666666667, test_loss: 0.3532146118581295, test_acc: 0.8848\n",
      "[0.8934, 0.8952, 0.8942] 3\n",
      "trainer_name: sgd, glob_circle: 2, lr: 0.001, ep: 3, train_loss: 0.1355723018658922, train_acc: 0.9505166666666667, test_loss: 0.3427006669342518, test_acc: 0.8891\n",
      "[0.8934, 0.8952, 0.8942] 4\n",
      "trainer_name: adam, glob_circle: 2, lr: 0.001, ep: 4, train_loss: 0.15141329369012346, train_acc: 0.9432333333333334, test_loss: 0.3584202740341425, test_acc: 0.8839\n",
      "[0.8934, 0.8952, 0.8942] 5\n",
      "trainer_name: sgd, glob_circle: 3, lr: 0.0005, ep: 0, train_loss: 0.12366639244112562, train_acc: 0.9549666666666666, test_loss: 0.33364069778472183, test_acc: 0.8878\n",
      "[0.8934, 0.8952, 0.8942] 0\n",
      "trainer_name: adam, glob_circle: 3, lr: 0.0005, ep: 1, train_loss: 0.11228558265782417, train_acc: 0.95945, test_loss: 0.33815927281975744, test_acc: 0.8953\n",
      "[0.8934, 0.8952, 0.8942] 1\n",
      "trainer_name: adam, glob_circle: 3, lr: 0.0005, ep: 2, train_loss: 0.09792507354249345, train_acc: 0.9648833333333333, test_loss: 0.3648218844085932, test_acc: 0.8917\n",
      "[0.8952, 0.8942, 0.8953] 0\n",
      "trainer_name: sgd, glob_circle: 3, lr: 0.0005, ep: 3, train_loss: 0.0841671735682386, train_acc: 0.9706833333333333, test_loss: 0.3550972644239664, test_acc: 0.8933\n",
      "[0.8952, 0.8942, 0.8953] 1\n",
      "trainer_name: adam, glob_circle: 3, lr: 0.0005, ep: 4, train_loss: 0.09111668282049767, train_acc: 0.9675833333333334, test_loss: 0.3676689390093088, test_acc: 0.8909\n",
      "[0.8952, 0.8942, 0.8953] 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c05ada55c7c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m           \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m           \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m           \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    #torch.nn.Dropout(p=0.15),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.BatchNorm1d(784),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(256),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    #torch.nn.Dropout(p=0.3),\n",
    "    torch.nn.BatchNorm1d(64),\n",
    "    torch.nn.Linear(64, 10)\n",
    ")\n",
    "print(model)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "l_lr = 0.001\n",
    "num_glob_circle = 100\n",
    "num_epochs = 10\n",
    "\n",
    "l_prev_acc = [0]*3 # array to gather statistics of last 5 steps' test-accuracy to do smth - change trainer or learning rate\n",
    "l_tries_max = 5 # max count of tries to not changing learning rate and to change trainer\n",
    "l_tries = 0 # count of tries\n",
    "trainer, trainer_name = torch.optim.Adam(model.parameters(), lr=l_lr), 'adam'\n",
    "#trainer, trainer_name = torch.optim.SGD(model.parameters(), lr=l_lr), 'sgd'\n",
    "for glob_circle in range(num_glob_circle):\n",
    "\n",
    "  for ep in range(num_epochs):\n",
    "      train_iters, train_passed  = 0, 0\n",
    "      train_loss, train_acc = 0., 0.\n",
    "      \n",
    "      for X, y in train:\n",
    "          trainer.zero_grad()\n",
    "          y_pred = model(X)\n",
    "          l = loss(y_pred, y)\n",
    "          l.backward()\n",
    "          trainer.step()\n",
    "          train_loss += l.item()\n",
    "          train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "          train_iters += 1\n",
    "          train_passed += len(X)\n",
    "      \n",
    "      test_iters, test_passed  = 0, 0\n",
    "      test_loss, test_acc = 0., 0.\n",
    "      for X, y in test:\n",
    "          y_pred = model(X)\n",
    "          l = loss(y_pred, y)\n",
    "          test_loss += l.item()\n",
    "          test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "          test_iters += 1\n",
    "          test_passed += len(X)\n",
    "          \n",
    "      print(\"trainer_name: {}, glob_circle: {}, lr: {}, ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "          trainer_name, glob_circle, l_lr, ep, train_loss / train_iters, train_acc / train_passed,\n",
    "          test_loss / test_iters, test_acc / test_passed)\n",
    "      )\n",
    "      print(l_prev_acc, l_tries)\n",
    "      \n",
    "      if test_acc / test_passed > l_prev_acc[0]:\n",
    "        l_prev_acc.append(test_acc / test_passed)\n",
    "        del l_prev_acc[0]\n",
    "        l_tries -= 1 if l_tries > 0 else 0\n",
    "      else:\n",
    "        l_tries += 1\n",
    "        if trainer_name == 'sgd':\n",
    "            trainer, trainer_name = torch.optim.Adam(model.parameters(), lr=l_lr), 'adam'\n",
    "        else:\n",
    "            trainer, trainer_name = torch.optim.SGD(model.parameters(), lr=l_lr), 'sgd'\n",
    "\n",
    "        if l_tries > l_tries_max:\n",
    "            l_lr = l_lr / 2\n",
    "            l_tries = 0\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
