{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции Активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:09:22.893628Z",
     "start_time": "2019-11-11T21:09:22.601085Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:09:23.354771Z",
     "start_time": "2019-11-11T21:09:23.352084Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:09:24.378405Z",
     "start_time": "2019-11-11T21:09:24.375592Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:16:31.637628Z",
     "start_time": "2019-11-11T21:16:31.634962Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:16:31.969721Z",
     "start_time": "2019-11-11T21:16:31.966998Z"
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:16:32.441551Z",
     "start_time": "2019-11-11T21:16:32.438570Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:23:40.230893Z",
     "start_time": "2019-11-11T21:23:40.189035Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = tv.datasets.MNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.MNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:23:40.795687Z",
     "start_time": "2019-11-11T21:23:40.672226Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f59d2944320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dataset[0][0].numpy().reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:23:41.010511Z",
     "start_time": "2019-11-11T21:23:41.003495Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:23:41.308317Z",
     "start_time": "2019-11-11T21:23:41.304926Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-11T21:23:41.448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.4309862635871197, train_acc: 0.87585, test_loss: 0.24166723182424904, test_acc: 0.925\n",
      "ep: 1, train_loss: 0.18669128712028898, train_acc: 0.9456166666666667, test_loss: 0.17019917559809983, test_acc: 0.946\n",
      "ep: 2, train_loss: 0.13498719413863852, train_acc: 0.9612666666666667, test_loss: 0.13370721803512425, test_acc: 0.9589\n",
      "ep: 3, train_loss: 0.10659929060951827, train_acc: 0.9696333333333333, test_loss: 0.11125230329344049, test_acc: 0.9669\n",
      "ep: 4, train_loss: 0.08795927095603435, train_acc: 0.9753333333333334, test_loss: 0.09857246611500159, test_acc: 0.9699\n",
      "ep: 5, train_loss: 0.07452432454583492, train_acc: 0.97955, test_loss: 0.08938360598986037, test_acc: 0.9717\n",
      "ep: 6, train_loss: 0.06431908229405575, train_acc: 0.9823, test_loss: 0.08353505390696228, test_acc: 0.9729\n",
      "ep: 7, train_loss: 0.05623939800809356, train_acc: 0.9847833333333333, test_loss: 0.07920555018936284, test_acc: 0.9737\n",
      "ep: 8, train_loss: 0.04950333736797279, train_acc: 0.9867, test_loss: 0.07614953365991824, test_acc: 0.9752\n",
      "ep: 9, train_loss: 0.043945326687807734, train_acc: 0.9886166666666667, test_loss: 0.07378618250077125, test_acc: 0.9761\n"
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика - попробуйте заменить SGD на Adam и RMSProp. Увеличиться ли скорость сходимости?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.25080450120917025, train_acc: 0.92355, test_loss: 0.1264035524567589, test_acc: 0.9585\n",
      "ep: 1, train_loss: 0.10340622668451768, train_acc: 0.9681, test_loss: 0.10760416284319944, test_acc: 0.9658\n",
      "ep: 2, train_loss: 0.07058307981673391, train_acc: 0.9779333333333333, test_loss: 0.11538263393449597, test_acc: 0.9667\n",
      "ep: 3, train_loss: 0.06191025254059028, train_acc: 0.98, test_loss: 0.10727834534591238, test_acc: 0.9708\n",
      "ep: 4, train_loss: 0.05120745299066952, train_acc: 0.9836666666666667, test_loss: 0.13070226087147602, test_acc: 0.9671\n",
      "ep: 5, train_loss: 0.043769214202976844, train_acc: 0.9855833333333334, test_loss: 0.12335631331879995, test_acc: 0.9717\n",
      "ep: 6, train_loss: 0.04284865043938477, train_acc: 0.98615, test_loss: 0.12813780421729462, test_acc: 0.9715\n",
      "ep: 7, train_loss: 0.04052430184240988, train_acc: 0.9875, test_loss: 0.14356009380444448, test_acc: 0.971\n",
      "ep: 8, train_loss: 0.04228798452284227, train_acc: 0.9873333333333333, test_loss: 0.13803879640656191, test_acc: 0.9729\n",
      "ep: 9, train_loss: 0.04438784128956576, train_acc: 0.9863666666666666, test_loss: 0.18368928472552853, test_acc: 0.9663\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 10)\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 2.279527799119341, train_acc: 0.12528333333333333, test_loss: 2.2109234809875487, test_acc: 0.1415\n",
      "ep: 1, train_loss: 2.211874538279594, train_acc: 0.15306666666666666, test_loss: 3.869586092233658, test_acc: 0.0985\n",
      "ep: 2, train_loss: 2.19831867420927, train_acc: 0.16286666666666666, test_loss: 2.3614763498306273, test_acc: 0.1136\n",
      "ep: 3, train_loss: 2.195650422826726, train_acc: 0.1582, test_loss: 2.2275931745767594, test_acc: 0.1685\n",
      "ep: 4, train_loss: 2.1597589396415873, train_acc: 0.16863333333333333, test_loss: 2.173701900243759, test_acc: 0.1608\n",
      "ep: 5, train_loss: 2.181040886615185, train_acc: 0.16638333333333333, test_loss: 2.1918744534254073, test_acc: 0.1623\n",
      "ep: 6, train_loss: 2.1424456687683753, train_acc: 0.17105, test_loss: 2.1645167589187624, test_acc: 0.1736\n",
      "ep: 7, train_loss: 2.1898340879602634, train_acc: 0.1609, test_loss: 2.1706851541996004, test_acc: 0.167\n",
      "ep: 8, train_loss: 2.1712705617255352, train_acc: 0.17318333333333333, test_loss: 2.1508590638637544, test_acc: 0.1647\n",
      "ep: 9, train_loss: 2.1487310186345527, train_acc: 0.1697, test_loss: 2.236396837234497, test_acc: 0.1689\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.RMSprop(model.parameters(), lr=0.2)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика - попробуйте сделать больше слоев в сети  - увеличиться ли качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.305743956359777, train_acc: 0.9070833333333334, test_loss: 0.17579629695974291, test_acc: 0.9429\n",
      "ep: 1, train_loss: 0.1120194943106555, train_acc: 0.96565, test_loss: 0.10864541182527318, test_acc: 0.9653\n",
      "ep: 2, train_loss: 0.07058025718607167, train_acc: 0.9780666666666666, test_loss: 0.10698354791093152, test_acc: 0.9687\n",
      "ep: 3, train_loss: 0.052564371367322005, train_acc: 0.9837666666666667, test_loss: 0.10261412283871323, test_acc: 0.9702\n",
      "ep: 4, train_loss: 0.046156433185047294, train_acc: 0.9852, test_loss: 0.09760832715473952, test_acc: 0.9745\n",
      "ep: 5, train_loss: 0.04277153908215622, train_acc: 0.9854166666666667, test_loss: 0.10411228489829227, test_acc: 0.9727\n",
      "ep: 6, train_loss: 0.03663195989520705, train_acc: 0.988, test_loss: 0.1072450497027603, test_acc: 0.9735\n",
      "ep: 7, train_loss: 0.028125066130838178, train_acc: 0.9906666666666667, test_loss: 0.1368720044614747, test_acc: 0.9674\n",
      "ep: 8, train_loss: 0.02545388491175316, train_acc: 0.9919666666666667, test_loss: 0.12457743720078725, test_acc: 0.9736\n",
      "ep: 9, train_loss: 0.023851916111109064, train_acc: 0.99195, test_loss: 0.13611416689600447, test_acc: 0.9727\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 10)\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практика - попробуйте добавить регуляризацию, dropout и/или batchnorm-слои. Увеличится ли качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.36129778772592547, train_acc: 0.89165, test_loss: 0.184076386410743, test_acc: 0.9423\n",
      "ep: 1, train_loss: 0.15256415629323491, train_acc: 0.9554666666666667, test_loss: 0.13766718762926758, test_acc: 0.9584\n",
      "ep: 2, train_loss: 0.11100380987008202, train_acc: 0.9676666666666667, test_loss: 0.1257155285682529, test_acc: 0.9636\n",
      "ep: 3, train_loss: 0.09043403954264966, train_acc: 0.9735333333333334, test_loss: 0.11387960741994903, test_acc: 0.9666\n",
      "ep: 4, train_loss: 0.0779563533142209, train_acc: 0.9762166666666666, test_loss: 0.10550365889430395, test_acc: 0.971\n",
      "ep: 5, train_loss: 0.0672012742569155, train_acc: 0.9790833333333333, test_loss: 0.10992527747730492, test_acc: 0.968\n",
      "ep: 6, train_loss: 0.06073801579112385, train_acc: 0.9812, test_loss: 0.09906497797346674, test_acc: 0.9723\n",
      "ep: 7, train_loss: 0.058899030592688854, train_acc: 0.9806333333333334, test_loss: 0.1216526282747509, test_acc: 0.971\n",
      "ep: 8, train_loss: 0.048996903998975425, train_acc: 0.9847666666666667, test_loss: 0.12073852482299116, test_acc: 0.9696\n",
      "ep: 9, train_loss: 0.04781002031361803, train_acc: 0.98485, test_loss: 0.14461662246612833, test_acc: 0.9688\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.1),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.3),\n",
    "    torch.nn.Linear(64, 10)\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    #torch.nn.Dropout(p=0.1),\n",
    "    #torch.nn.BatchNorm1d(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    #torch.nn.Dropout(p=0.3),\n",
    "    torch.nn.Linear(64, 10)\n",
    ")\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:16:32.441551Z",
     "start_time": "2019-11-11T21:16:32.438570Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T21:23:40.230893Z",
     "start_time": "2019-11-11T21:23:40.189035Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26421880 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26427392it [00:03, 7315627.56it/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29515 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 153556.21it/s]           \n",
      "  0%|          | 0/4422102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4423680it [00:00, 5454038.95it/s]                             \n",
      "8192it [00:00, 63595.70it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
    "test_dataset = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
    "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten()\n",
      "  (1): BatchNorm1d(784, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "  (7): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "ep: 0, train_loss: 0.47749507972534666, train_acc: 0.8384666666666667, test_loss: 0.39075520932674407, test_acc: 0.8596\n",
      "ep: 1, train_loss: 0.32645542685021745, train_acc: 0.8814, test_loss: 0.3596591901034117, test_acc: 0.8704\n",
      "ep: 2, train_loss: 0.28162298322992124, train_acc: 0.8973833333333333, test_loss: 0.34650804474949837, test_acc: 0.8762\n",
      "ep: 3, train_loss: 0.2508826104884452, train_acc: 0.90885, test_loss: 0.33921575024724004, test_acc: 0.8777\n",
      "ep: 4, train_loss: 0.22665399636359926, train_acc: 0.91725, test_loss: 0.33578277081251146, test_acc: 0.8852\n",
      "ep: 5, train_loss: 0.2051618941603823, train_acc: 0.9258166666666666, test_loss: 0.3431892301887274, test_acc: 0.8834\n",
      "ep: 6, train_loss: 0.18818504772287734, train_acc: 0.93095, test_loss: 0.3491486681625247, test_acc: 0.8826\n",
      "ep: 7, train_loss: 0.17380319976426187, train_acc: 0.9372666666666667, test_loss: 0.35760375633835795, test_acc: 0.8779\n",
      "ep: 8, train_loss: 0.15490418114560717, train_acc: 0.9440833333333334, test_loss: 0.36806964622810484, test_acc: 0.8805\n",
      "ep: 9, train_loss: 0.14434447413746346, train_acc: 0.9476166666666667, test_loss: 0.37780798617750405, test_acc: 0.8806\n"
     ]
    }
   ],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    #torch.nn.Dropout(p=0.15),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.BatchNorm1d(784),\n",
    "    torch.nn.Linear(784, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(256),\n",
    "    torch.nn.Linear(256, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    #torch.nn.Dropout(p=0.3),\n",
    "    torch.nn.BatchNorm1d(64),\n",
    "    torch.nn.Linear(64, 10)\n",
    ")\n",
    "print(model)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "trainer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.1373146324557193, train_acc: 0.9496833333333333, test_loss: 0.3505227343179286, test_acc: 0.8878\n",
      "ep: 1, train_loss: 0.12304901021750683, train_acc: 0.9555, test_loss: 0.34542447738349435, test_acc: 0.8889\n",
      "ep: 2, train_loss: 0.11637508004428225, train_acc: 0.9582666666666667, test_loss: 0.343269501067698, test_acc: 0.8895\n",
      "ep: 3, train_loss: 0.11178575484676564, train_acc: 0.9602333333333334, test_loss: 0.34235506048426034, test_acc: 0.8904\n",
      "ep: 4, train_loss: 0.10826269449230204, train_acc: 0.9618833333333333, test_loss: 0.34213403444737195, test_acc: 0.8901\n",
      "ep: 5, train_loss: 0.10538770381281985, train_acc: 0.9632, test_loss: 0.3423170618712902, test_acc: 0.8904\n",
      "ep: 6, train_loss: 0.10291622787555482, train_acc: 0.9644, test_loss: 0.3427231838926673, test_acc: 0.8907\n",
      "ep: 7, train_loss: 0.10076225126518848, train_acc: 0.9652, test_loss: 0.3433120801113546, test_acc: 0.8912\n",
      "ep: 8, train_loss: 0.09884346362916713, train_acc: 0.9660833333333333, test_loss: 0.34400788238272073, test_acc: 0.8914\n",
      "ep: 9, train_loss: 0.09710131092591488, train_acc: 0.9668333333333333, test_loss: 0.34473010096698997, test_acc: 0.8908\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, train_loss: 0.11713985025565675, train_acc: 0.9583666666666667, test_loss: 0.3757933542132378, test_acc: 0.8883\n",
      "ep: 1, train_loss: 0.09806427877951175, train_acc: 0.9661666666666666, test_loss: 0.39390783868730067, test_acc: 0.8873\n",
      "ep: 2, train_loss: 0.08442707049085739, train_acc: 0.9720333333333333, test_loss: 0.40874220747500656, test_acc: 0.8861\n",
      "ep: 3, train_loss: 0.07356942698676536, train_acc: 0.9765, test_loss: 0.43760117068886756, test_acc: 0.8849\n",
      "ep: 4, train_loss: 0.06315076211665539, train_acc: 0.9797666666666667, test_loss: 0.4673977652564645, test_acc: 0.8841\n",
      "ep: 5, train_loss: 0.0551292428984604, train_acc: 0.9830666666666666, test_loss: 0.4877183087170124, test_acc: 0.8843\n",
      "ep: 6, train_loss: 0.04905917105126254, train_acc: 0.9855, test_loss: 0.5215748764574528, test_acc: 0.882\n",
      "ep: 7, train_loss: 0.04808309673272232, train_acc: 0.9850333333333333, test_loss: 0.5312640104442835, test_acc: 0.8828\n",
      "ep: 8, train_loss: 0.04618289453631386, train_acc: 0.9855833333333334, test_loss: 0.5533147484064103, test_acc: 0.8815\n",
      "ep: 9, train_loss: 0.04204662515920527, train_acc: 0.9861333333333333, test_loss: 0.5548096239566803, test_acc: 0.8835\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    train_iters, train_passed  = 0, 0\n",
    "    train_loss, train_acc = 0., 0.\n",
    "    \n",
    "    for X, y in train:\n",
    "        trainer.zero_grad()\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        train_loss += l.item()\n",
    "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        train_iters += 1\n",
    "        train_passed += len(X)\n",
    "    \n",
    "    test_iters, test_passed  = 0, 0\n",
    "    test_loss, test_acc = 0., 0.\n",
    "    for X, y in test:\n",
    "        y_pred = model(X)\n",
    "        l = loss(y_pred, y)\n",
    "        test_loss += l.item()\n",
    "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        test_iters += 1\n",
    "        test_passed += len(X)\n",
    "        \n",
    "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "        ep, train_loss / train_iters, train_acc / train_passed,\n",
    "        test_loss / test_iters, test_acc / test_passed)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
