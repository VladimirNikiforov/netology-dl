{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "name": "Практика3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5L3r22CqjR",
        "colab_type": "text"
      },
      "source": [
        "## Функции Активации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:09:22.893628Z",
          "start_time": "2019-11-11T21:09:22.601085Z"
        },
        "id": "hE2Y1P8xCqjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:09:23.354771Z",
          "start_time": "2019-11-11T21:09:23.352084Z"
        },
        "id": "yt-sqCOrCqjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:09:24.378405Z",
          "start_time": "2019-11-11T21:09:24.375592Z"
        },
        "id": "6Py6VAd6Cqjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Jh5Mu6Cqjk",
        "colab_type": "text"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CqulALnCqjl",
        "colab_type": "text"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:16:31.637628Z",
          "start_time": "2019-11-11T21:16:31.634962Z"
        },
        "id": "a7y9iknXCqjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision as tv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:16:31.969721Z",
          "start_time": "2019-11-11T21:16:31.966998Z"
        },
        "id": "0f6Xbal9Cqjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:16:32.441551Z",
          "start_time": "2019-11-11T21:16:32.438570Z"
        },
        "id": "WKd9_jq4Cqjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:23:40.230893Z",
          "start_time": "2019-11-11T21:23:40.189035Z"
        },
        "id": "6-q_d0aACqj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9643e193-979c-4faa-92c4-6cb0c41b143f"
      },
      "source": [
        "train_dataset = tv.datasets.MNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
        "test_dataset = tv.datasets.MNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
        "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
        "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:03, 3114871.48it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 49682.38it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:02, 825289.08it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 18782.55it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:23:40.795687Z",
          "start_time": "2019-11-11T21:23:40.672226Z"
        },
        "scrolled": true,
        "id": "Bh1WuIzKCqj4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "c5004b0b-af8f-49ee-af11-fc480c76c3a7"
      },
      "source": [
        "plt.imshow(train_dataset[0][0].numpy().reshape(28,28), cmap='gray')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f48fbb89240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qr3p6tMDCqj-",
        "colab_type": "text"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:23:41.010511Z",
          "start_time": "2019-11-11T21:23:41.003495Z"
        },
        "id": "C_0AkjSFCqkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 10)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:23:41.308317Z",
          "start_time": "2019-11-11T21:23:41.304926Z"
        },
        "id": "f6tVTURRCqkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = torch.nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
        "num_epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-11T21:23:41.448Z"
        },
        "id": "s0wdGLi0CqkI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "65bdc2ca-eacd-46e0-dd19-3f1099c4827e"
      },
      "source": [
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.43806211437950743, train_acc: 0.8718666666666667, test_loss: 0.2446914380416274, test_acc: 0.9246\n",
            "ep: 1, train_loss: 0.1899401124804578, train_acc: 0.9447666666666666, test_loss: 0.18029865566641093, test_acc: 0.9434\n",
            "ep: 2, train_loss: 0.13776352656807037, train_acc: 0.9607166666666667, test_loss: 0.14155601998791098, test_acc: 0.9566\n",
            "ep: 3, train_loss: 0.108440090442433, train_acc: 0.9691833333333333, test_loss: 0.1189381544245407, test_acc: 0.9634\n",
            "ep: 4, train_loss: 0.08936783809294092, train_acc: 0.97505, test_loss: 0.1025958024780266, test_acc: 0.9682\n",
            "ep: 5, train_loss: 0.07575275483442113, train_acc: 0.9790333333333333, test_loss: 0.09272746562492103, test_acc: 0.9708\n",
            "ep: 6, train_loss: 0.06530946366132256, train_acc: 0.9821333333333333, test_loss: 0.08560335382353515, test_acc: 0.9738\n",
            "ep: 7, train_loss: 0.057037416942655406, train_acc: 0.9844666666666667, test_loss: 0.08168934433488176, test_acc: 0.9746\n",
            "ep: 8, train_loss: 0.05032605525066561, train_acc: 0.9865166666666667, test_loss: 0.0771480223746039, test_acc: 0.976\n",
            "ep: 9, train_loss: 0.04469668928811208, train_acc: 0.98855, test_loss: 0.07496577300480567, test_acc: 0.9766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xJNAUW0CqkN",
        "colab_type": "text"
      },
      "source": [
        "## Практика - попробуйте заменить SGD на Adam и RMSProp. Увеличиться ли скорость сходимости?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93iz7EL0CqkP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "df8aeb6e-a3a6-46bc-ea98-84fd9efc942b"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 10)\n",
        ")\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.25028374117422614, train_acc: 0.9248333333333333, test_loss: 0.14524812470190226, test_acc: 0.9539\n",
            "ep: 1, train_loss: 0.10566356325165388, train_acc: 0.96785, test_loss: 0.09756288226344623, test_acc: 0.9691\n",
            "ep: 2, train_loss: 0.07091532532284234, train_acc: 0.9780166666666666, test_loss: 0.13142999320989474, test_acc: 0.9626\n",
            "ep: 3, train_loss: 0.0618626899462431, train_acc: 0.9799, test_loss: 0.13872262883305667, test_acc: 0.9642\n",
            "ep: 4, train_loss: 0.052518137221402944, train_acc: 0.9831166666666666, test_loss: 0.1162714437537943, test_acc: 0.97\n",
            "ep: 5, train_loss: 0.04739823080698385, train_acc: 0.9845166666666667, test_loss: 0.12663812247265013, test_acc: 0.9704\n",
            "ep: 6, train_loss: 0.04352814682581006, train_acc: 0.9860666666666666, test_loss: 0.13051886866844598, test_acc: 0.9714\n",
            "ep: 7, train_loss: 0.0395371701285996, train_acc: 0.9880166666666667, test_loss: 0.1652839066778597, test_acc: 0.9659\n",
            "ep: 8, train_loss: 0.04211744891375581, train_acc: 0.9868333333333333, test_loss: 0.16096903946017846, test_acc: 0.9686\n",
            "ep: 9, train_loss: 0.040226099297820415, train_acc: 0.9878333333333333, test_loss: 0.15957525459652971, test_acc: 0.972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnfnBFg6CqkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cd02d6a6-45ef-43b8-b444-028efc124adf"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 10)\n",
        ")\n",
        "trainer = torch.optim.RMSprop(model.parameters(), lr=0.01)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.8479294154238194, train_acc: 0.8963166666666667, test_loss: 0.4534246899187565, test_acc: 0.8762\n",
            "ep: 1, train_loss: 0.1529344073873251, train_acc: 0.9561, test_loss: 0.4551741316914558, test_acc: 0.8904\n",
            "ep: 2, train_loss: 0.10873233966846416, train_acc: 0.9673166666666667, test_loss: 0.6513886108994484, test_acc: 0.8819\n",
            "ep: 3, train_loss: 0.0875918420309082, train_acc: 0.9736833333333333, test_loss: 0.11950736017897726, test_acc: 0.9661\n",
            "ep: 4, train_loss: 0.0677890812552118, train_acc: 0.97875, test_loss: 0.14801583015359937, test_acc: 0.9616\n",
            "ep: 5, train_loss: 0.05612889126914137, train_acc: 0.9826666666666667, test_loss: 0.1873030059505254, test_acc: 0.9533\n",
            "ep: 6, train_loss: 0.04825423864172177, train_acc: 0.9846833333333334, test_loss: 0.17157349165063351, test_acc: 0.9624\n",
            "ep: 7, train_loss: 0.04109333714648606, train_acc: 0.9868166666666667, test_loss: 0.14383858686778694, test_acc: 0.9665\n",
            "ep: 8, train_loss: 0.03831634336706013, train_acc: 0.9882166666666666, test_loss: 0.24084399179555477, test_acc: 0.9525\n",
            "ep: 9, train_loss: 0.032275091025586976, train_acc: 0.9898333333333333, test_loss: 0.13351730371359735, test_acc: 0.9724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1vMcOOICqkY",
        "colab_type": "text"
      },
      "source": [
        "## Практика - попробуйте сделать больше слоев в сети  - увеличиться ли качество?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBidI-cXCqka",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "558213a1-f3a4-4b6e-8757-9bd9999458be"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(64, 10)\n",
        ")\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.30942338667809965, train_acc: 0.9070333333333334, test_loss: 0.16914815615164117, test_acc: 0.9438\n",
            "ep: 1, train_loss: 0.11299491632888292, train_acc: 0.96625, test_loss: 0.11641185453627259, test_acc: 0.9638\n",
            "ep: 2, train_loss: 0.07363993383230681, train_acc: 0.97715, test_loss: 0.09968600271095056, test_acc: 0.9677\n",
            "ep: 3, train_loss: 0.053048910838650896, train_acc: 0.9837333333333333, test_loss: 0.1010510757618249, test_acc: 0.9701\n",
            "ep: 4, train_loss: 0.045486284489248026, train_acc: 0.9856333333333334, test_loss: 0.1108421052718768, test_acc: 0.969\n",
            "ep: 5, train_loss: 0.04181569814483853, train_acc: 0.9857, test_loss: 0.09996080518449162, test_acc: 0.9728\n",
            "ep: 6, train_loss: 0.03682065683991668, train_acc: 0.9876166666666667, test_loss: 0.08869844721257322, test_acc: 0.9753\n",
            "ep: 7, train_loss: 0.031404564066968385, train_acc: 0.9891, test_loss: 0.09583330173682043, test_acc: 0.9746\n",
            "ep: 8, train_loss: 0.02652743095234829, train_acc: 0.9912666666666666, test_loss: 0.10857202940096614, test_acc: 0.973\n",
            "ep: 9, train_loss: 0.0225563175687715, train_acc: 0.9920833333333333, test_loss: 0.13326928808376123, test_acc: 0.969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sV7JTEkCqke",
        "colab_type": "text"
      },
      "source": [
        "## Практика - попробуйте добавить регуляризацию, dropout и/или batchnorm-слои. Увеличится ли качество?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9I1c9hSCqkg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "bddd2322-800d-4cf4-a4ac-3c7666d357a2"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.1),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Linear(784, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(256, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Dropout(p=0.3),\n",
        "    torch.nn.Linear(64, 10)\n",
        ")\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.3579540642969152, train_acc: 0.8937333333333334, test_loss: 0.17939673298969866, test_acc: 0.9447\n",
            "ep: 1, train_loss: 0.1450917762406963, train_acc: 0.9575333333333333, test_loss: 0.1269842053297907, test_acc: 0.9608\n",
            "ep: 2, train_loss: 0.10702699663394943, train_acc: 0.9682666666666667, test_loss: 0.13167636535363272, test_acc: 0.9622\n",
            "ep: 3, train_loss: 0.08938399732628084, train_acc: 0.9727666666666667, test_loss: 0.11100658595241839, test_acc: 0.9681\n",
            "ep: 4, train_loss: 0.07860253241428353, train_acc: 0.9756166666666667, test_loss: 0.10561397053970722, test_acc: 0.9704\n",
            "ep: 5, train_loss: 0.06809952283634785, train_acc: 0.9786, test_loss: 0.10699371444206918, test_acc: 0.9723\n",
            "ep: 6, train_loss: 0.05914921403486044, train_acc: 0.9816666666666667, test_loss: 0.1226057239662623, test_acc: 0.9667\n",
            "ep: 7, train_loss: 0.05542128389265309, train_acc: 0.9823666666666667, test_loss: 0.12663102153892397, test_acc: 0.9667\n",
            "ep: 8, train_loss: 0.05355906089609291, train_acc: 0.9831833333333333, test_loss: 0.1219490060492717, test_acc: 0.9701\n",
            "ep: 9, train_loss: 0.04782502976979347, train_acc: 0.9844333333333334, test_loss: 0.1377371742448304, test_acc: 0.9683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "belBYJhLCqkk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "fdecf43a-c3d5-43e9-9a81-cc89078d11f1"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    #torch.nn.Dropout(p=0.1),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.BatchNorm1d(784),\n",
        "    torch.nn.Linear(784, 256),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm1d(256),\n",
        "    torch.nn.Linear(256, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    #torch.nn.Dropout(p=0.3),\n",
        "    torch.nn.BatchNorm1d(64),\n",
        "    torch.nn.Linear(64, 10)\n",
        ")\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.21633996729283256, train_acc: 0.9364, test_loss: 0.12494889532681555, test_acc: 0.9601\n",
            "ep: 1, train_loss: 0.08471490665675795, train_acc: 0.9741833333333333, test_loss: 0.10266675311140716, test_acc: 0.9687\n",
            "ep: 2, train_loss: 0.048913608308128534, train_acc: 0.9845666666666667, test_loss: 0.11336418355349451, test_acc: 0.9677\n",
            "ep: 3, train_loss: 0.03248597920371933, train_acc: 0.9899166666666667, test_loss: 0.12084827182261507, test_acc: 0.9686\n",
            "ep: 4, train_loss: 0.028793690075225968, train_acc: 0.9906833333333334, test_loss: 0.12104182565817609, test_acc: 0.9696\n",
            "ep: 5, train_loss: 0.022199561977647542, train_acc: 0.9922166666666666, test_loss: 0.12448932444676757, test_acc: 0.9721\n",
            "ep: 6, train_loss: 0.01972281233100419, train_acc: 0.9929833333333333, test_loss: 0.11615704157447908, test_acc: 0.9734\n",
            "ep: 7, train_loss: 0.012182401520190166, train_acc: 0.99615, test_loss: 0.12037885885392825, test_acc: 0.9757\n",
            "ep: 8, train_loss: 0.010989860560437863, train_acc: 0.9966666666666667, test_loss: 0.1306464398468961, test_acc: 0.9723\n",
            "ep: 9, train_loss: 0.011111101889710357, train_acc: 0.9962666666666666, test_loss: 0.13941145233117141, test_acc: 0.9732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o9El6n_Cqkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhAtIGV2Cqkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reg6VoJHD52u",
        "colab_type": "text"
      },
      "source": [
        "### Домашнее задание"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:09:22.893628Z",
          "start_time": "2019-11-11T21:09:22.601085Z"
        },
        "colab_type": "code",
        "id": "z_W0g-z-YYFI",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision as tv\n",
        "import time\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:16:32.441551Z",
          "start_time": "2019-11-11T21:16:32.438570Z"
        },
        "id": "bzc0unAbCqk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-11T21:23:40.230893Z",
          "start_time": "2019-11-11T21:23:40.189035Z"
        },
        "id": "ZGnqyzoNCqlF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "90129626-9367-403f-c513-2f88b4dc42cd"
      },
      "source": [
        "train_dataset = tv.datasets.FashionMNIST('.', train=True, transform=tv.transforms.ToTensor(), download=True)\n",
        "test_dataset = tv.datasets.FashionMNIST('.', train=False, transform=tv.transforms.ToTensor(), download=True)\n",
        "train = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:04, 5834265.81it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/train-images-idx3-ubyte.gz to ./FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 40409.59it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:02, 1699516.01it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 15310.09it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iivdTBr6CqlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "ee8a2c60-d9a3-4173-c6f5-5de5ac200be6"
      },
      "source": [
        "'''model = torch.nn.Sequential(\n",
        "    #torch.nn.Dropout(p=0.1),\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.BatchNorm1d(784),\n",
        "    torch.nn.Linear(784, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm1d(64),\n",
        "    torch.nn.Linear(64, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm1d(128),\n",
        "    torch.nn.Linear(128, 128),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.BatchNorm1d(128),\n",
        "    torch.nn.Linear(128, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    #torch.nn.Dropout(p=0.3),\n",
        "    torch.nn.BatchNorm1d(64),\n",
        "    torch.nn.Linear(64, 10)\n",
        ")'''\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Dropout(p=0.15),\n",
        "    #torch.nn.BatchNorm1d(784),\n",
        "    torch.nn.Linear(784, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    #torch.nn.BatchNorm1d(256),\n",
        "    #torch.nn.Linear(256, 128),\n",
        "    #torch.nn.ReLU(),\n",
        "    #torch.nn.BatchNorm1d(128),\n",
        "    #torch.nn.Linear(128, 64),\n",
        "    #torch.nn.ReLU(),\n",
        "    #torch.nn.Dropout(p=0.3),\n",
        "    #torch.nn.BatchNorm1d(64),\n",
        "    torch.nn.Linear(64, 10)\n",
        ")\n",
        "print(model)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "LEARNING_RATE = 0.005\n",
        "num_glob_circle = 20\n",
        "num_epochs = 50\n",
        "\n",
        "l_prev_loss = [100,100,100,100]\n",
        "l_lr = LEARNING_RATE\n",
        "#trainer = torch.optim.Adam(model.parameters(), lr=l_lr)\n",
        "trainer, trainer_name = torch.optim.Adam(model.parameters(), lr=l_lr), 'adam'\n",
        "for glob_circle in range(num_glob_circle):\n",
        "  #trainer = torch.optim.Adam(model.parameters(), lr=l_lr)\n",
        "  #trainer, trainer_name = torch.optim.SGD(model.parameters(), lr=l_lr), 'sgd'\n",
        "\n",
        "  for ep in range(num_epochs):\n",
        "      train_iters, train_passed  = 0, 0\n",
        "      train_loss, train_acc = 0., 0.\n",
        "      \n",
        "      for X, y in train:\n",
        "          trainer.zero_grad()\n",
        "          y_pred = model(X)\n",
        "          l = loss(y_pred, y)\n",
        "          l.backward()\n",
        "          trainer.step()\n",
        "          train_loss += l.item()\n",
        "          train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "          train_iters += 1\n",
        "          train_passed += len(X)\n",
        "      \n",
        "      test_iters, test_passed  = 0, 0\n",
        "      test_loss, test_acc = 0., 0.\n",
        "      for X, y in test:\n",
        "          y_pred = model(X)\n",
        "          l = loss(y_pred, y)\n",
        "          test_loss += l.item()\n",
        "          test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "          test_iters += 1\n",
        "          test_passed += len(X)\n",
        "          \n",
        "      print(\"trainer_name: {}, glob_circle: {}, lr: {}, ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "          trainer_name, glob_circle, l_lr, ep, train_loss / train_iters, train_acc / train_passed,\n",
        "          test_loss / test_iters, test_acc / test_passed)\n",
        "      )\n",
        "      if test_loss / test_iters < max(l_prev_loss):\n",
        "        l_prev_loss.append(test_loss / test_iters)\n",
        "        del l_prev_loss[0]\n",
        "        #print(l_prev_loss)\n",
        "      else:\n",
        "        if ep != 0:\n",
        "          l_lr = l_lr / 2\n",
        "          break\n",
        "        '''else:\n",
        "          if trainer_name == 'sgd':\n",
        "            trainer, trainer_name = torch.optim.Adam(model.parameters(), lr=l_lr), 'adam'\n",
        "          else:\n",
        "            trainer, trainer_name = torch.optim.SGD(model.parameters(), lr=l_lr), 'sgd'\n",
        "        #break'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Flatten()\n",
            "  (1): Dropout(p=0.15, inplace=False)\n",
            "  (2): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 0, train_loss: 0.5816112281160152, train_acc: 0.7955333333333333, test_loss: 0.5049769878387451, test_acc: 0.8148\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 1, train_loss: 0.4400159423655652, train_acc: 0.84345, test_loss: 0.4487044714391232, test_acc: 0.8372\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 2, train_loss: 0.4022969107678596, train_acc: 0.8545, test_loss: 0.4424269303679466, test_acc: 0.8453\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 3, train_loss: 0.37435932387696935, train_acc: 0.8628833333333333, test_loss: 0.41862376146018504, test_acc: 0.8485\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 4, train_loss: 0.3639631019627794, train_acc: 0.8651666666666666, test_loss: 0.41721331477165224, test_acc: 0.8508\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 5, train_loss: 0.3518508757682557, train_acc: 0.8685166666666667, test_loss: 0.39553353264927865, test_acc: 0.8575\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 6, train_loss: 0.33823649680360834, train_acc: 0.8751666666666666, test_loss: 0.40234433114528656, test_acc: 0.857\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 7, train_loss: 0.33367249185734604, train_acc: 0.8766666666666667, test_loss: 0.3846606358885765, test_acc: 0.8578\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 8, train_loss: 0.32486749075828714, train_acc: 0.8800166666666667, test_loss: 0.4042417161166668, test_acc: 0.8524\n",
            "trainer_name: adam, glob_circle: 0, lr: 0.005, ep: 9, train_loss: 0.3238291692860583, train_acc: 0.8792, test_loss: 0.38707307539880276, test_acc: 0.8627\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dc5a1e8a977b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m           \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m           \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRnz0-WYUhmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3769eb85-dabf-4bea-f5b2-4c6eed255cb7"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Flatten(),\n",
        "    torch.nn.Dropout(p=0.3),\n",
        "    #torch.nn.BatchNorm1d(784),\n",
        "    torch.nn.Linear(784, 784*2),\n",
        "    torch.nn.ReLU(),\n",
        "    #torch.nn.BatchNorm1d(256),\n",
        "    torch.nn.Linear(784*2, 784),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(784, 64),\n",
        "    torch.nn.ReLU(),\n",
        "    #torch.nn.Dropout(p=0.3),\n",
        "    #torch.nn.BatchNorm1d(64),\n",
        "    torch.nn.Linear(64, 10)\n",
        ")\n",
        "\n",
        "print(model)\n",
        "loss = torch.nn.CrossEntropyLoss()\n",
        "num_epochs = 100\n",
        "\n",
        "l_lr = 0.00001\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=l_lr)\n",
        "#trainer, trainer_name = torch.optim.SGD(model.parameters(), lr=l_lr), 'sgd'\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Flatten()\n",
            "  (1): Dropout(p=0.3, inplace=False)\n",
            "  (2): Linear(in_features=784, out_features=1568, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=1568, out_features=784, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=784, out_features=64, bias=True)\n",
            "  (7): ReLU()\n",
            "  (8): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n",
            "ep: 0, train_loss: 2.007032090045036, train_acc: 0.43735, test_loss: 1.4971646994352341, test_acc: 0.6138\n",
            "ep: 1, train_loss: 1.1166599174763294, train_acc: 0.66105, test_loss: 0.912967962026596, test_acc: 0.6696\n",
            "ep: 2, train_loss: 0.8151928721590245, train_acc: 0.7052833333333334, test_loss: 0.7762050539255142, test_acc: 0.7208\n",
            "ep: 3, train_loss: 0.7213461307769126, train_acc: 0.7451666666666666, test_loss: 0.7152278959751129, test_acc: 0.7481\n",
            "ep: 4, train_loss: 0.6694677223550513, train_acc: 0.7672833333333333, test_loss: 0.6703276425600052, test_acc: 0.7641\n",
            "ep: 5, train_loss: 0.6304133516676882, train_acc: 0.7810166666666667, test_loss: 0.6337151490151882, test_acc: 0.7732\n",
            "ep: 6, train_loss: 0.607036828360659, train_acc: 0.7893833333333333, test_loss: 0.6140177071094512, test_acc: 0.7813\n",
            "ep: 7, train_loss: 0.5839965832994339, train_acc: 0.7970333333333334, test_loss: 0.5961761921644211, test_acc: 0.7867\n",
            "ep: 8, train_loss: 0.5678050913709275, train_acc: 0.80315, test_loss: 0.5861659221351146, test_acc: 0.7933\n",
            "ep: 9, train_loss: 0.5574512929358381, train_acc: 0.8048833333333333, test_loss: 0.5731511034071446, test_acc: 0.7972\n",
            "ep: 10, train_loss: 0.5467369457508655, train_acc: 0.8081166666666667, test_loss: 0.5613346621394157, test_acc: 0.8006\n",
            "ep: 11, train_loss: 0.5380014605978702, train_acc: 0.8104166666666667, test_loss: 0.5587842792272568, test_acc: 0.7982\n",
            "ep: 12, train_loss: 0.5282764415791694, train_acc: 0.8158333333333333, test_loss: 0.5499719470739365, test_acc: 0.8015\n",
            "ep: 13, train_loss: 0.5224148505545677, train_acc: 0.8162833333333334, test_loss: 0.5390601769089699, test_acc: 0.8074\n",
            "ep: 14, train_loss: 0.5160794130031099, train_acc: 0.8178666666666666, test_loss: 0.5376775443553925, test_acc: 0.8071\n",
            "ep: 15, train_loss: 0.5095225097017085, train_acc: 0.8191333333333334, test_loss: 0.5321432262659073, test_acc: 0.8123\n",
            "ep: 16, train_loss: 0.5053892358820489, train_acc: 0.8225, test_loss: 0.53324726074934, test_acc: 0.812\n",
            "ep: 17, train_loss: 0.5002511578671476, train_acc: 0.8235666666666667, test_loss: 0.5194186389446258, test_acc: 0.8124\n",
            "ep: 18, train_loss: 0.49682319164276123, train_acc: 0.82505, test_loss: 0.513856279104948, test_acc: 0.8151\n",
            "ep: 19, train_loss: 0.4923330030542739, train_acc: 0.8265, test_loss: 0.5244231559336185, test_acc: 0.8122\n",
            "ep: 20, train_loss: 0.4889838917458311, train_acc: 0.8272166666666667, test_loss: 0.5243370778858661, test_acc: 0.8148\n",
            "ep: 21, train_loss: 0.4847710464863067, train_acc: 0.8272333333333334, test_loss: 0.5176085188984871, test_acc: 0.8201\n",
            "ep: 22, train_loss: 0.4792003053299924, train_acc: 0.83115, test_loss: 0.5126907907426357, test_acc: 0.8168\n",
            "ep: 23, train_loss: 0.4790761261544329, train_acc: 0.8306166666666667, test_loss: 0.49399268440902233, test_acc: 0.8212\n",
            "ep: 24, train_loss: 0.4763094805656595, train_acc: 0.8311833333333334, test_loss: 0.5043071031570434, test_acc: 0.8231\n",
            "ep: 25, train_loss: 0.4695483020011415, train_acc: 0.8325666666666667, test_loss: 0.4905867151916027, test_acc: 0.8227\n",
            "ep: 26, train_loss: 0.47035925515154575, train_acc: 0.8331333333333333, test_loss: 0.4994527705013752, test_acc: 0.8251\n",
            "ep: 27, train_loss: 0.46708019185573496, train_acc: 0.83485, test_loss: 0.48728649839758875, test_acc: 0.8257\n",
            "ep: 28, train_loss: 0.4647718737734125, train_acc: 0.8365666666666667, test_loss: 0.4943365715444088, test_acc: 0.8221\n",
            "ep: 29, train_loss: 0.46041024200459746, train_acc: 0.83695, test_loss: 0.49991580843925476, test_acc: 0.8252\n",
            "ep: 30, train_loss: 0.45916724369881, train_acc: 0.8363166666666667, test_loss: 0.48157251477241514, test_acc: 0.8277\n",
            "ep: 31, train_loss: 0.4559195376457052, train_acc: 0.8382, test_loss: 0.48068545013666153, test_acc: 0.8298\n",
            "ep: 32, train_loss: 0.45437900728367747, train_acc: 0.8398833333333333, test_loss: 0.4895068071782589, test_acc: 0.8271\n",
            "ep: 33, train_loss: 0.45339225086760016, train_acc: 0.8383666666666667, test_loss: 0.4756755378097296, test_acc: 0.8286\n",
            "ep: 34, train_loss: 0.44948501751777975, train_acc: 0.84085, test_loss: 0.48902268782258035, test_acc: 0.8276\n",
            "ep: 35, train_loss: 0.44811061379757333, train_acc: 0.8415666666666667, test_loss: 0.4769929341971874, test_acc: 0.8298\n",
            "ep: 36, train_loss: 0.44660180490067664, train_acc: 0.8417833333333333, test_loss: 0.484406416118145, test_acc: 0.8263\n",
            "ep: 37, train_loss: 0.44359713425027564, train_acc: 0.8428333333333333, test_loss: 0.48753526657819746, test_acc: 0.8335\n",
            "ep: 38, train_loss: 0.4446033448614973, train_acc: 0.8416666666666667, test_loss: 0.4728958234190941, test_acc: 0.8312\n",
            "ep: 39, train_loss: 0.440657322077041, train_acc: 0.84275, test_loss: 0.4785191312432289, test_acc: 0.8304\n",
            "ep: 40, train_loss: 0.4406784284622111, train_acc: 0.8428166666666667, test_loss: 0.47428725808858874, test_acc: 0.835\n",
            "ep: 41, train_loss: 0.4352161412543439, train_acc: 0.846, test_loss: 0.46428839638829233, test_acc: 0.8331\n",
            "ep: 42, train_loss: 0.4383055589300521, train_acc: 0.8454666666666667, test_loss: 0.47114063128829003, test_acc: 0.8335\n",
            "ep: 43, train_loss: 0.4351836401097318, train_acc: 0.84455, test_loss: 0.46758694350719454, test_acc: 0.8331\n",
            "ep: 44, train_loss: 0.4333806343535159, train_acc: 0.84555, test_loss: 0.4587946444749832, test_acc: 0.8328\n",
            "ep: 45, train_loss: 0.4325048056054623, train_acc: 0.84695, test_loss: 0.46190947890281675, test_acc: 0.8338\n",
            "ep: 46, train_loss: 0.43062006336577396, train_acc: 0.8471333333333333, test_loss: 0.45855226516723635, test_acc: 0.8383\n",
            "ep: 47, train_loss: 0.42782429266483224, train_acc: 0.84765, test_loss: 0.45969761461019515, test_acc: 0.8377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaFNfNHQot3N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c689ba73-d5e4-4f07-b901-96f12794e303"
      },
      "source": [
        "l_lr = 0.001\n",
        "#trainer = torch.optim.Adam(model.parameters(), lr=l_lr)\n",
        "trainer = torch.optim.SGD(model.parameters(), lr=l_lr)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.26618962633912846, train_acc: 0.9020166666666667, test_loss: 0.3850274384021759, test_acc: 0.8717\n",
            "ep: 1, train_loss: 0.2601424402604669, train_acc: 0.9043, test_loss: 0.38364130407571795, test_acc: 0.8747\n",
            "ep: 2, train_loss: 0.2597699494937719, train_acc: 0.9052666666666667, test_loss: 0.39676312953233717, test_acc: 0.877\n",
            "ep: 3, train_loss: 0.2585946951629752, train_acc: 0.9043666666666667, test_loss: 0.39009893834590914, test_acc: 0.871\n",
            "ep: 4, train_loss: 0.2600662257459204, train_acc: 0.9049833333333334, test_loss: 0.3877578377723694, test_acc: 0.8721\n",
            "ep: 5, train_loss: 0.2595129057259883, train_acc: 0.905, test_loss: 0.38549071103334426, test_acc: 0.8736\n",
            "ep: 6, train_loss: 0.2565924741201482, train_acc: 0.9055833333333333, test_loss: 0.38994742184877396, test_acc: 0.8744\n",
            "ep: 7, train_loss: 0.2544286431649984, train_acc: 0.9065833333333333, test_loss: 0.3868996188044548, test_acc: 0.8725\n",
            "ep: 8, train_loss: 0.25689120527546283, train_acc: 0.90705, test_loss: 0.3911189824342728, test_acc: 0.8753\n",
            "ep: 9, train_loss: 0.25353346866066173, train_acc: 0.9053166666666667, test_loss: 0.39414503872394563, test_acc: 0.8774\n",
            "ep: 10, train_loss: 0.2566759904562417, train_acc: 0.9063166666666667, test_loss: 0.3843193233013153, test_acc: 0.8744\n",
            "ep: 11, train_loss: 0.2535729520654274, train_acc: 0.9065166666666666, test_loss: 0.3829274967312813, test_acc: 0.8759\n",
            "ep: 12, train_loss: 0.25340246952186196, train_acc: 0.9056, test_loss: 0.38919345289468765, test_acc: 0.8775\n",
            "ep: 13, train_loss: 0.25479324098865863, train_acc: 0.9068833333333334, test_loss: 0.37583017647266387, test_acc: 0.8782\n",
            "ep: 14, train_loss: 0.2523039365976544, train_acc: 0.9066833333333333, test_loss: 0.3859441354870796, test_acc: 0.8731\n",
            "ep: 15, train_loss: 0.25595929183192173, train_acc: 0.9063, test_loss: 0.37769705802202225, test_acc: 0.874\n",
            "ep: 16, train_loss: 0.25284706826432274, train_acc: 0.9060166666666667, test_loss: 0.39111886769533155, test_acc: 0.8719\n",
            "ep: 17, train_loss: 0.2557285415419078, train_acc: 0.9058833333333334, test_loss: 0.3790262594819069, test_acc: 0.877\n",
            "ep: 18, train_loss: 0.25325176031407665, train_acc: 0.9049333333333334, test_loss: 0.38625641316175463, test_acc: 0.8759\n",
            "ep: 19, train_loss: 0.25476026547662284, train_acc: 0.9063333333333333, test_loss: 0.3829731434583664, test_acc: 0.8726\n",
            "ep: 20, train_loss: 0.25305653565515907, train_acc: 0.9062166666666667, test_loss: 0.38295449018478395, test_acc: 0.8735\n",
            "ep: 21, train_loss: 0.25580478611133867, train_acc: 0.9042333333333333, test_loss: 0.39016580432653425, test_acc: 0.8724\n",
            "ep: 22, train_loss: 0.25341226804559513, train_acc: 0.9062166666666667, test_loss: 0.38873699456453326, test_acc: 0.8716\n",
            "ep: 23, train_loss: 0.2551560323622267, train_acc: 0.9052666666666667, test_loss: 0.3920090824365616, test_acc: 0.8748\n",
            "ep: 24, train_loss: 0.2536454061835499, train_acc: 0.9062166666666667, test_loss: 0.38784942775964737, test_acc: 0.872\n",
            "ep: 25, train_loss: 0.2542672270940522, train_acc: 0.9057166666666666, test_loss: 0.3818962350487709, test_acc: 0.8744\n",
            "ep: 26, train_loss: 0.25163646282280905, train_acc: 0.9059333333333334, test_loss: 0.3871423050761223, test_acc: 0.8754\n",
            "ep: 27, train_loss: 0.2515169904898789, train_acc: 0.9056166666666666, test_loss: 0.3908678725361824, test_acc: 0.8729\n",
            "ep: 28, train_loss: 0.24961319073277005, train_acc: 0.9063833333333333, test_loss: 0.3820691600441933, test_acc: 0.873\n",
            "ep: 29, train_loss: 0.2536231345039303, train_acc: 0.90525, test_loss: 0.3740891873836517, test_acc: 0.8758\n",
            "ep: 30, train_loss: 0.2524279604776431, train_acc: 0.9063333333333333, test_loss: 0.3831400781869888, test_acc: 0.8754\n",
            "ep: 31, train_loss: 0.2520367268283488, train_acc: 0.9076166666666666, test_loss: 0.376638725399971, test_acc: 0.8744\n",
            "ep: 32, train_loss: 0.25292279482898067, train_acc: 0.9062166666666667, test_loss: 0.3866489753127098, test_acc: 0.872\n",
            "ep: 33, train_loss: 0.2522897090194589, train_acc: 0.9055666666666666, test_loss: 0.37791589498519895, test_acc: 0.8755\n",
            "ep: 34, train_loss: 0.250517688565335, train_acc: 0.9065, test_loss: 0.3803895577788353, test_acc: 0.8774\n",
            "ep: 35, train_loss: 0.2530879342960099, train_acc: 0.90545, test_loss: 0.38611439019441607, test_acc: 0.8725\n",
            "ep: 36, train_loss: 0.2518699314382117, train_acc: 0.9061333333333333, test_loss: 0.38644847422838213, test_acc: 0.874\n",
            "ep: 37, train_loss: 0.25307928947574, train_acc: 0.9062, test_loss: 0.3785498648881912, test_acc: 0.8743\n",
            "ep: 38, train_loss: 0.2502901532639891, train_acc: 0.9074333333333333, test_loss: 0.38652461022138596, test_acc: 0.874\n",
            "ep: 39, train_loss: 0.25152048013978084, train_acc: 0.9062666666666667, test_loss: 0.38353439420461655, test_acc: 0.8731\n",
            "ep: 40, train_loss: 0.25178773019273404, train_acc: 0.9059833333333334, test_loss: 0.3820218503475189, test_acc: 0.8752\n",
            "ep: 41, train_loss: 0.24930883097951695, train_acc: 0.9056333333333333, test_loss: 0.3828808099031448, test_acc: 0.8749\n",
            "ep: 42, train_loss: 0.2511420016319065, train_acc: 0.9071333333333333, test_loss: 0.3727179244160652, test_acc: 0.874\n",
            "ep: 43, train_loss: 0.25015141991740564, train_acc: 0.9064166666666666, test_loss: 0.38606421947479247, test_acc: 0.8763\n",
            "ep: 44, train_loss: 0.2549881459040157, train_acc: 0.906, test_loss: 0.3938411623239517, test_acc: 0.8698\n",
            "ep: 45, train_loss: 0.25080106041188965, train_acc: 0.90705, test_loss: 0.3803751230239868, test_acc: 0.8776\n",
            "ep: 46, train_loss: 0.2554701809155739, train_acc: 0.9064333333333333, test_loss: 0.395112419128418, test_acc: 0.8721\n",
            "ep: 47, train_loss: 0.25082977457066713, train_acc: 0.9063166666666667, test_loss: 0.40120674967765807, test_acc: 0.8725\n",
            "ep: 48, train_loss: 0.2534799561914751, train_acc: 0.9056166666666666, test_loss: 0.38517794758081436, test_acc: 0.8731\n",
            "ep: 49, train_loss: 0.2529811987937507, train_acc: 0.9062666666666667, test_loss: 0.3723526239395142, test_acc: 0.8767\n",
            "ep: 50, train_loss: 0.2508972248030921, train_acc: 0.9063666666666667, test_loss: 0.3832350820302963, test_acc: 0.8749\n",
            "ep: 51, train_loss: 0.2521400308962596, train_acc: 0.9077166666666666, test_loss: 0.39344176799058916, test_acc: 0.8713\n",
            "ep: 52, train_loss: 0.2512042415091547, train_acc: 0.9070333333333334, test_loss: 0.38343246579170226, test_acc: 0.8738\n",
            "ep: 53, train_loss: 0.25115757359791613, train_acc: 0.9052166666666667, test_loss: 0.38398542553186416, test_acc: 0.8754\n",
            "ep: 54, train_loss: 0.2514633872246338, train_acc: 0.9054833333333333, test_loss: 0.37838700860738755, test_acc: 0.8723\n",
            "ep: 55, train_loss: 0.24916624366226842, train_acc: 0.9070833333333334, test_loss: 0.37336838990449905, test_acc: 0.8782\n",
            "ep: 56, train_loss: 0.25322394565505496, train_acc: 0.9061, test_loss: 0.3742350563406944, test_acc: 0.8772\n",
            "ep: 57, train_loss: 0.2509195347458629, train_acc: 0.9066666666666666, test_loss: 0.3896395817399025, test_acc: 0.8715\n",
            "ep: 58, train_loss: 0.2535713505694422, train_acc: 0.9056833333333333, test_loss: 0.382025833427906, test_acc: 0.8762\n",
            "ep: 59, train_loss: 0.2508103395417585, train_acc: 0.9071666666666667, test_loss: 0.3763941958546638, test_acc: 0.8744\n",
            "ep: 60, train_loss: 0.2516581904837641, train_acc: 0.90745, test_loss: 0.38981869965791704, test_acc: 0.8793\n",
            "ep: 61, train_loss: 0.25214436271433105, train_acc: 0.9060833333333334, test_loss: 0.37323181331157684, test_acc: 0.875\n",
            "ep: 62, train_loss: 0.2537271270560006, train_acc: 0.90685, test_loss: 0.38643669039011, test_acc: 0.874\n",
            "ep: 63, train_loss: 0.2515069911793127, train_acc: 0.9061833333333333, test_loss: 0.38102935552597045, test_acc: 0.8745\n",
            "ep: 64, train_loss: 0.2531120402075477, train_acc: 0.9057166666666666, test_loss: 0.3684989407658577, test_acc: 0.8772\n",
            "ep: 65, train_loss: 0.251667754756192, train_acc: 0.9056333333333333, test_loss: 0.38069481700658797, test_acc: 0.8751\n",
            "ep: 66, train_loss: 0.2505901435674247, train_acc: 0.9072, test_loss: 0.37741780281066895, test_acc: 0.8742\n",
            "ep: 67, train_loss: 0.24912347897129544, train_acc: 0.9067166666666666, test_loss: 0.3793330579996109, test_acc: 0.8705\n",
            "ep: 68, train_loss: 0.2501581856759928, train_acc: 0.9062, test_loss: 0.3946225315332413, test_acc: 0.8718\n",
            "ep: 69, train_loss: 0.24786090509871306, train_acc: 0.90755, test_loss: 0.37446028292179107, test_acc: 0.8763\n",
            "ep: 70, train_loss: 0.24967871517953227, train_acc: 0.9074166666666666, test_loss: 0.3772258788347244, test_acc: 0.8749\n",
            "ep: 71, train_loss: 0.24777742879370512, train_acc: 0.9062333333333333, test_loss: 0.37432007640600207, test_acc: 0.8744\n",
            "ep: 72, train_loss: 0.2505897709121138, train_acc: 0.90715, test_loss: 0.3817752957344055, test_acc: 0.8769\n",
            "ep: 73, train_loss: 0.25166504135576345, train_acc: 0.9067666666666667, test_loss: 0.37068031579256056, test_acc: 0.8757\n",
            "ep: 74, train_loss: 0.252074770881968, train_acc: 0.9068833333333334, test_loss: 0.3735733777284622, test_acc: 0.8767\n",
            "ep: 75, train_loss: 0.2521962430012428, train_acc: 0.9061166666666667, test_loss: 0.3693428233265877, test_acc: 0.8782\n",
            "ep: 76, train_loss: 0.252273888658669, train_acc: 0.9068, test_loss: 0.3741087570786476, test_acc: 0.873\n",
            "ep: 77, train_loss: 0.24883605249352375, train_acc: 0.90775, test_loss: 0.3800505489110947, test_acc: 0.8741\n",
            "ep: 78, train_loss: 0.24814073880345133, train_acc: 0.9071166666666667, test_loss: 0.3696730613708496, test_acc: 0.8779\n",
            "ep: 79, train_loss: 0.24940015931250686, train_acc: 0.9075333333333333, test_loss: 0.3892301842570305, test_acc: 0.8759\n",
            "ep: 80, train_loss: 0.2523562603835332, train_acc: 0.9066333333333333, test_loss: 0.36945085898041724, test_acc: 0.8767\n",
            "ep: 81, train_loss: 0.24948284497200432, train_acc: 0.90725, test_loss: 0.3782099649310112, test_acc: 0.8783\n",
            "ep: 82, train_loss: 0.2527072579931405, train_acc: 0.9061, test_loss: 0.38258665800094604, test_acc: 0.8742\n",
            "ep: 83, train_loss: 0.2530906956832288, train_acc: 0.9060833333333334, test_loss: 0.36856476664543153, test_acc: 0.8743\n",
            "ep: 84, train_loss: 0.25327492833642634, train_acc: 0.9064166666666666, test_loss: 0.38441007286310197, test_acc: 0.8721\n",
            "ep: 85, train_loss: 0.24896109445115266, train_acc: 0.90645, test_loss: 0.38246281892061235, test_acc: 0.8766\n",
            "ep: 86, train_loss: 0.25050000240237025, train_acc: 0.9063666666666667, test_loss: 0.38232787847518923, test_acc: 0.8722\n",
            "ep: 87, train_loss: 0.2506621179439254, train_acc: 0.9068666666666667, test_loss: 0.37935646027326586, test_acc: 0.8757\n",
            "ep: 88, train_loss: 0.25300520721633557, train_acc: 0.90565, test_loss: 0.37545733749866483, test_acc: 0.8745\n",
            "ep: 89, train_loss: 0.250105344137903, train_acc: 0.9066166666666666, test_loss: 0.3736816719174385, test_acc: 0.8725\n",
            "ep: 90, train_loss: 0.2502711693109092, train_acc: 0.9061333333333333, test_loss: 0.38499933034181594, test_acc: 0.8723\n",
            "ep: 91, train_loss: 0.2485822114651486, train_acc: 0.90715, test_loss: 0.38116503953933717, test_acc: 0.8766\n",
            "ep: 92, train_loss: 0.2500008925542993, train_acc: 0.9073, test_loss: 0.37676714807748796, test_acc: 0.8748\n",
            "ep: 93, train_loss: 0.24954571587554478, train_acc: 0.9055666666666666, test_loss: 0.38115055561065675, test_acc: 0.873\n",
            "ep: 94, train_loss: 0.2506303931191816, train_acc: 0.9061666666666667, test_loss: 0.37832915037870407, test_acc: 0.8769\n",
            "ep: 95, train_loss: 0.25029415258411636, train_acc: 0.9059333333333334, test_loss: 0.38540748208761216, test_acc: 0.8729\n",
            "ep: 96, train_loss: 0.25122315095642866, train_acc: 0.9066, test_loss: 0.38597402125597, test_acc: 0.873\n",
            "ep: 97, train_loss: 0.2493468669511504, train_acc: 0.90695, test_loss: 0.3807528719305992, test_acc: 0.8743\n",
            "ep: 98, train_loss: 0.2473478002820985, train_acc: 0.9073666666666667, test_loss: 0.3742270529270172, test_acc: 0.8737\n",
            "ep: 99, train_loss: 0.2508469237867048, train_acc: 0.9065666666666666, test_loss: 0.37574563175439835, test_acc: 0.8789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULbc0r_cZdKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "b3881ad6-1a27-4f58-ad78-e1672fbea0dc"
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "l_lr = 0.001\n",
        "#trainer = torch.optim.Adam(model.parameters(), lr=l_lr)\n",
        "trainer = torch.optim.SGD(model.parameters(), lr=l_lr, momentum=0.1)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.2540524264765998, train_acc: 0.9055166666666666, test_loss: 0.38086457550525665, test_acc: 0.8765\n",
            "ep: 1, train_loss: 0.2501078529630677, train_acc: 0.9063166666666667, test_loss: 0.3734755277633667, test_acc: 0.8766\n",
            "ep: 2, train_loss: 0.24867442427045208, train_acc: 0.9063833333333333, test_loss: 0.3828242391347885, test_acc: 0.8752\n",
            "ep: 3, train_loss: 0.25161558629597647, train_acc: 0.90635, test_loss: 0.3845138520002365, test_acc: 0.8733\n",
            "ep: 4, train_loss: 0.25211743909423634, train_acc: 0.9074, test_loss: 0.3859805315732956, test_acc: 0.8756\n",
            "ep: 5, train_loss: 0.24676321863622988, train_acc: 0.9068666666666667, test_loss: 0.38657804727554324, test_acc: 0.8741\n",
            "ep: 6, train_loss: 0.2474490795600212, train_acc: 0.9071166666666667, test_loss: 0.3801821991801262, test_acc: 0.8732\n",
            "ep: 7, train_loss: 0.24925198456493475, train_acc: 0.9072166666666667, test_loss: 0.3771050006151199, test_acc: 0.8765\n",
            "ep: 8, train_loss: 0.2489212812003443, train_acc: 0.9074333333333333, test_loss: 0.3772375300526619, test_acc: 0.8724\n",
            "ep: 9, train_loss: 0.24902588078531168, train_acc: 0.9062833333333333, test_loss: 0.3815403550863266, test_acc: 0.8758\n",
            "ep: 10, train_loss: 0.25292357549829, train_acc: 0.9059666666666667, test_loss: 0.37215202301740646, test_acc: 0.8747\n",
            "ep: 11, train_loss: 0.24645032064389374, train_acc: 0.9081, test_loss: 0.3732477769255638, test_acc: 0.8737\n",
            "ep: 12, train_loss: 0.24755810302192882, train_acc: 0.9072166666666667, test_loss: 0.3815541058778763, test_acc: 0.873\n",
            "ep: 13, train_loss: 0.25048774632356935, train_acc: 0.90735, test_loss: 0.37371398955583573, test_acc: 0.8754\n",
            "ep: 14, train_loss: 0.2505784735588704, train_acc: 0.9062333333333333, test_loss: 0.38135316371917727, test_acc: 0.8733\n",
            "ep: 15, train_loss: 0.25035634295920195, train_acc: 0.90685, test_loss: 0.3780078709125519, test_acc: 0.8759\n",
            "ep: 16, train_loss: 0.2486589374178547, train_acc: 0.9075333333333333, test_loss: 0.37483215183019636, test_acc: 0.8776\n",
            "ep: 17, train_loss: 0.25019861189490655, train_acc: 0.90675, test_loss: 0.3800249546766281, test_acc: 0.876\n",
            "ep: 18, train_loss: 0.24737462060431303, train_acc: 0.90785, test_loss: 0.37911606431007383, test_acc: 0.8754\n",
            "ep: 19, train_loss: 0.24758347502704395, train_acc: 0.9072, test_loss: 0.371687775850296, test_acc: 0.8725\n",
            "ep: 20, train_loss: 0.24905448608984382, train_acc: 0.9068166666666667, test_loss: 0.36763947904109956, test_acc: 0.8775\n",
            "ep: 21, train_loss: 0.24985059199191756, train_acc: 0.90645, test_loss: 0.3775685980916023, test_acc: 0.8769\n",
            "ep: 22, train_loss: 0.24710342452182607, train_acc: 0.90785, test_loss: 0.3821204051375389, test_acc: 0.876\n",
            "ep: 23, train_loss: 0.24817329986115633, train_acc: 0.9072333333333333, test_loss: 0.3728707149624825, test_acc: 0.8748\n",
            "ep: 24, train_loss: 0.24927474002716904, train_acc: 0.9068166666666667, test_loss: 0.3798670321702957, test_acc: 0.8764\n",
            "ep: 25, train_loss: 0.24961569993677785, train_acc: 0.90595, test_loss: 0.3907112643122673, test_acc: 0.8734\n",
            "ep: 26, train_loss: 0.24868452713146047, train_acc: 0.9073166666666667, test_loss: 0.3888114497065544, test_acc: 0.8792\n",
            "ep: 27, train_loss: 0.24887476305840378, train_acc: 0.90795, test_loss: 0.3790254905819893, test_acc: 0.8747\n",
            "ep: 28, train_loss: 0.24846188900834423, train_acc: 0.9080833333333334, test_loss: 0.3845841445028782, test_acc: 0.8692\n",
            "ep: 29, train_loss: 0.251226665736255, train_acc: 0.9064, test_loss: 0.3848109841346741, test_acc: 0.8755\n",
            "ep: 30, train_loss: 0.24795564125149938, train_acc: 0.9081166666666667, test_loss: 0.38129593431949615, test_acc: 0.8722\n",
            "ep: 31, train_loss: 0.2501002850673966, train_acc: 0.9062333333333333, test_loss: 0.38443433940410615, test_acc: 0.8726\n",
            "ep: 32, train_loss: 0.24832576147075427, train_acc: 0.90795, test_loss: 0.38137099891901016, test_acc: 0.8781\n",
            "ep: 33, train_loss: 0.24857908424179434, train_acc: 0.9079666666666667, test_loss: 0.3785163536667824, test_acc: 0.8753\n",
            "ep: 34, train_loss: 0.25375924852945037, train_acc: 0.9058166666666667, test_loss: 0.38323248028755186, test_acc: 0.8739\n",
            "ep: 35, train_loss: 0.2501380619103626, train_acc: 0.9068333333333334, test_loss: 0.38130863606929777, test_acc: 0.8747\n",
            "ep: 36, train_loss: 0.2475467810438851, train_acc: 0.9067333333333333, test_loss: 0.38801153749227524, test_acc: 0.8759\n",
            "ep: 37, train_loss: 0.24669281229124232, train_acc: 0.9080333333333334, test_loss: 0.3795367494225502, test_acc: 0.8748\n",
            "ep: 38, train_loss: 0.24960314330913252, train_acc: 0.9058166666666667, test_loss: 0.36480152457952497, test_acc: 0.8795\n",
            "ep: 39, train_loss: 0.24862541498269064, train_acc: 0.9072166666666667, test_loss: 0.38462247848510744, test_acc: 0.8744\n",
            "ep: 40, train_loss: 0.2510779662910154, train_acc: 0.90805, test_loss: 0.3802702873945236, test_acc: 0.874\n",
            "ep: 41, train_loss: 0.24865366316447823, train_acc: 0.90615, test_loss: 0.37895705699920657, test_acc: 0.8746\n",
            "ep: 42, train_loss: 0.2471776517518496, train_acc: 0.908, test_loss: 0.37695069760084154, test_acc: 0.8735\n",
            "ep: 43, train_loss: 0.24720114671577842, train_acc: 0.9063166666666667, test_loss: 0.37492576986551285, test_acc: 0.8784\n",
            "ep: 44, train_loss: 0.24970900557808956, train_acc: 0.9068, test_loss: 0.38033163696527483, test_acc: 0.871\n",
            "ep: 45, train_loss: 0.24910765722141429, train_acc: 0.9061666666666667, test_loss: 0.37426078915596006, test_acc: 0.8746\n",
            "ep: 46, train_loss: 0.2509221884153657, train_acc: 0.9066166666666666, test_loss: 0.3862159937620163, test_acc: 0.8728\n",
            "ep: 47, train_loss: 0.24899699980929746, train_acc: 0.9065333333333333, test_loss: 0.3826183989644051, test_acc: 0.8732\n",
            "ep: 48, train_loss: 0.25069385678586315, train_acc: 0.90695, test_loss: 0.36476860493421553, test_acc: 0.875\n",
            "ep: 49, train_loss: 0.24879085967096232, train_acc: 0.9065833333333333, test_loss: 0.37348559498786926, test_acc: 0.8756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3B-c-CGaET1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "78a79d4a-d25a-4860-f027-a31fb200375f"
      },
      "source": [
        "num_epochs = 50\n",
        "\n",
        "l_lr = 0.00005\n",
        "trainer = torch.optim.Adam(model.parameters(), lr=l_lr)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.2491744129081904, train_acc: 0.9060666666666667, test_loss: 0.3760410353541374, test_acc: 0.8767\n",
            "ep: 1, train_loss: 0.2466297494405407, train_acc: 0.9081166666666667, test_loss: 0.3634828686714172, test_acc: 0.8771\n",
            "ep: 2, train_loss: 0.2495763375092361, train_acc: 0.9067166666666666, test_loss: 0.3713590770959854, test_acc: 0.8759\n",
            "ep: 3, train_loss: 0.24473253915370521, train_acc: 0.9077833333333334, test_loss: 0.37639252841472626, test_acc: 0.8747\n",
            "ep: 4, train_loss: 0.24424421597840423, train_acc: 0.90885, test_loss: 0.3741656497120857, test_acc: 0.8762\n",
            "ep: 5, train_loss: 0.24642195161116326, train_acc: 0.9077333333333333, test_loss: 0.3758451610803604, test_acc: 0.8779\n",
            "ep: 6, train_loss: 0.24604736937809799, train_acc: 0.9077833333333334, test_loss: 0.3768702119588852, test_acc: 0.8746\n",
            "ep: 7, train_loss: 0.24177607742406554, train_acc: 0.90915, test_loss: 0.3730737090110779, test_acc: 0.8778\n",
            "ep: 8, train_loss: 0.24376479713088375, train_acc: 0.90835, test_loss: 0.3709833353757858, test_acc: 0.8775\n",
            "ep: 9, train_loss: 0.2418719032558344, train_acc: 0.9088666666666667, test_loss: 0.3714046776294708, test_acc: 0.8774\n",
            "ep: 10, train_loss: 0.2416604374677448, train_acc: 0.9083833333333333, test_loss: 0.370180481672287, test_acc: 0.8784\n",
            "ep: 11, train_loss: 0.24168002201338945, train_acc: 0.90925, test_loss: 0.37030023634433745, test_acc: 0.8783\n",
            "ep: 12, train_loss: 0.23940041939080772, train_acc: 0.91005, test_loss: 0.3649471715092659, test_acc: 0.8807\n",
            "ep: 13, train_loss: 0.2425259446441117, train_acc: 0.909, test_loss: 0.367509751021862, test_acc: 0.8797\n",
            "ep: 14, train_loss: 0.24036453335972155, train_acc: 0.9095166666666666, test_loss: 0.37096236646175385, test_acc: 0.8735\n",
            "ep: 15, train_loss: 0.23952270299196243, train_acc: 0.9104666666666666, test_loss: 0.3694180101156235, test_acc: 0.8779\n",
            "ep: 16, train_loss: 0.24024573637772415, train_acc: 0.9106166666666666, test_loss: 0.3689236268401146, test_acc: 0.8783\n",
            "ep: 17, train_loss: 0.24091175451117047, train_acc: 0.909, test_loss: 0.3789911031723022, test_acc: 0.8782\n",
            "ep: 18, train_loss: 0.2368994364041393, train_acc: 0.9096833333333333, test_loss: 0.3699096113443375, test_acc: 0.8769\n",
            "ep: 19, train_loss: 0.24186848104000092, train_acc: 0.9099, test_loss: 0.37724406123161314, test_acc: 0.8773\n",
            "ep: 20, train_loss: 0.23695053602174176, train_acc: 0.9110333333333334, test_loss: 0.37614563554525376, test_acc: 0.876\n",
            "ep: 21, train_loss: 0.24001321689052096, train_acc: 0.9089, test_loss: 0.3752609819173813, test_acc: 0.8767\n",
            "ep: 22, train_loss: 0.24129118239980632, train_acc: 0.90845, test_loss: 0.36796384751796724, test_acc: 0.8778\n",
            "ep: 23, train_loss: 0.23799573529069706, train_acc: 0.9102, test_loss: 0.37725940346717834, test_acc: 0.876\n",
            "ep: 24, train_loss: 0.23926934764041738, train_acc: 0.9101833333333333, test_loss: 0.37041003406047823, test_acc: 0.8748\n",
            "ep: 25, train_loss: 0.2391195792262837, train_acc: 0.9107666666666666, test_loss: 0.37660214155912397, test_acc: 0.8801\n",
            "ep: 26, train_loss: 0.2384230164905726, train_acc: 0.911, test_loss: 0.3624841570854187, test_acc: 0.8815\n",
            "ep: 27, train_loss: 0.23809027318227088, train_acc: 0.9101333333333333, test_loss: 0.368542905151844, test_acc: 0.8794\n",
            "ep: 28, train_loss: 0.23695484144707857, train_acc: 0.9109333333333334, test_loss: 0.3633240759372711, test_acc: 0.8798\n",
            "ep: 29, train_loss: 0.2380751496401884, train_acc: 0.9097166666666666, test_loss: 0.3699911147356033, test_acc: 0.8781\n",
            "ep: 30, train_loss: 0.23698484859729219, train_acc: 0.91015, test_loss: 0.3756134331226349, test_acc: 0.8784\n",
            "ep: 31, train_loss: 0.23727461506249542, train_acc: 0.9094166666666667, test_loss: 0.3734925463795662, test_acc: 0.8788\n",
            "ep: 32, train_loss: 0.23868066564959994, train_acc: 0.9080666666666667, test_loss: 0.3651264667510986, test_acc: 0.8801\n",
            "ep: 33, train_loss: 0.23891682531368935, train_acc: 0.9103166666666667, test_loss: 0.36588960587978364, test_acc: 0.8821\n",
            "ep: 34, train_loss: 0.23888286746154397, train_acc: 0.9093333333333333, test_loss: 0.3669582739472389, test_acc: 0.8804\n",
            "ep: 35, train_loss: 0.23696306739318168, train_acc: 0.9097833333333334, test_loss: 0.3719596818089485, test_acc: 0.8777\n",
            "ep: 36, train_loss: 0.23633995071306066, train_acc: 0.9103, test_loss: 0.3763144388794899, test_acc: 0.8814\n",
            "ep: 37, train_loss: 0.23636598910315562, train_acc: 0.911, test_loss: 0.3651788532733917, test_acc: 0.8826\n",
            "ep: 38, train_loss: 0.23726278429819367, train_acc: 0.9108, test_loss: 0.36393180340528486, test_acc: 0.8757\n",
            "ep: 39, train_loss: 0.23573329388085057, train_acc: 0.9093333333333333, test_loss: 0.3760282963514328, test_acc: 0.8726\n",
            "ep: 40, train_loss: 0.23349558410503096, train_acc: 0.9111833333333333, test_loss: 0.3620190843939781, test_acc: 0.8793\n",
            "ep: 41, train_loss: 0.2390201929009567, train_acc: 0.91035, test_loss: 0.3633255407214165, test_acc: 0.8805\n",
            "ep: 42, train_loss: 0.23509852588176727, train_acc: 0.9108166666666667, test_loss: 0.35837762206792834, test_acc: 0.879\n",
            "ep: 43, train_loss: 0.23313180887598103, train_acc: 0.9112333333333333, test_loss: 0.37546250969171524, test_acc: 0.8767\n",
            "ep: 44, train_loss: 0.2375093987685139, train_acc: 0.9107166666666666, test_loss: 0.3668911963701248, test_acc: 0.8805\n",
            "ep: 45, train_loss: 0.23757033024804067, train_acc: 0.9112333333333333, test_loss: 0.3786316603422165, test_acc: 0.8766\n",
            "ep: 46, train_loss: 0.2339347616090613, train_acc: 0.9109333333333334, test_loss: 0.36814445853233335, test_acc: 0.8757\n",
            "ep: 47, train_loss: 0.23558314851785112, train_acc: 0.9097166666666666, test_loss: 0.36786470413208006, test_acc: 0.8816\n",
            "ep: 48, train_loss: 0.23652448290485448, train_acc: 0.9116333333333333, test_loss: 0.37754698991775515, test_acc: 0.8757\n",
            "ep: 49, train_loss: 0.23627261047141027, train_acc: 0.9098666666666667, test_loss: 0.3607071593403816, test_acc: 0.8814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR37Ftc-aFM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5H985ypaFJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSwU5-mdaFFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wX7bm4buaFEc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGA0tUppaE_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQFImLZfIkG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glob_circle: 0, lr: 0.1, ep: 0, train_loss: 0.6065994061054067, train_acc: 0.78185, test_loss: 0.5109195582568645, test_acc: 0.8192\n",
        "glob_circle: 0, lr: 0.1, ep: 1, train_loss: 0.45688008412401726, train_acc: 0.8389333333333333, test_loss: 0.47057330831885336, test_acc: 0.8353\n",
        "glob_circle: 0, lr: 0.1, ep: 2, train_loss: 0.41267927796282666, train_acc: 0.8524333333333334, test_loss: 0.4547542527318001, test_acc: 0.8443\n",
        "glob_circle: 0, lr: 0.1, ep: 3, train_loss: 0.38714229680122214, train_acc: 0.86135, test_loss: 0.43902769275009634, test_acc: 0.8552\n",
        "glob_circle: 0, lr: 0.1, ep: 4, train_loss: 0.37191634140116103, train_acc: 0.8665666666666667, test_loss: 0.4391955006867647, test_acc: 0.8546\n",
        "glob_circle: 1, lr: 0.05, ep: 0, train_loss: 0.33139099402630584, train_acc: 0.8809166666666667, test_loss: 0.4078574441373348, test_acc: 0.8591\n",
        "glob_circle: 1, lr: 0.05, ep: 1, train_loss: 0.3039758097618184, train_acc: 0.8893833333333333, test_loss: 0.4119338944554329, test_acc: 0.8614\n",
        "glob_circle: 2, lr: 0.025, ep: 0, train_loss: 0.2657677289653332, train_acc: 0.9021166666666667, test_loss: 0.37955835089087486, test_acc: 0.8736\n",
        "glob_circle: 2, lr: 0.025, ep: 1, train_loss: 0.24686847608140175, train_acc: 0.9092, test_loss: 0.37656555995345115, test_acc: 0.8756\n",
        "glob_circle: 2, lr: 0.025, ep: 2, train_loss: 0.23773925615117905, train_acc: 0.9114, test_loss: 0.3899051796644926, test_acc: 0.8755\n",
        "glob_circle: 3, lr: 0.0125, ep: 0, train_loss: 0.21213517192196338, train_acc: 0.9209666666666667, test_loss: 0.3982499998062849, test_acc: 0.8781\n",
        "glob_circle: 4, lr: 0.00625, ep: 0, train_loss: 0.19116364787233636, train_acc: 0.9281666666666667, test_loss: 0.38535967357456685, test_acc: 0.8824\n",
        "glob_circle: 5, lr: 0.003125, ep: 0, train_loss: 0.17959381801650878, train_acc: 0.9324333333333333, test_loss: 0.39340553879737855, test_acc: 0.8824\n",
        "glob_circle: 6, lr: 0.0015625, ep: 0, train_loss: 0.17236472457013233, train_acc: 0.9351166666666667, test_loss: 0.38345172852277754, test_acc: 0.8849\n",
        "glob_circle: 7, lr: 0.00078125, ep: 0, train_loss: 0.17008166627046908, train_acc: 0.9362833333333334, test_loss: 0.38595688827335833, test_acc: 0.8871\n",
        "glob_circle: 8, lr: 0.000390625, ep: 0, train_loss: 0.16788862344432384, train_acc: 0.9373166666666667, test_loss: 0.3814737504348159, test_acc: 0.8872\n",
        "glob_circle: 9, lr: 0.0001953125, ep: 0, train_loss: 0.16730869532899653, train_acc: 0.9372166666666667, test_loss: 0.4014470908790827, test_acc: 0.8844\n",
        "glob_circle: 10, lr: 9.765625e-05, ep: 0, train_loss: 0.16659931426352642, train_acc: 0.9380666666666667, test_loss: 0.3864532221108675, test_acc: 0.8842"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5n55SdzCqlO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ed006c1b-50ef-4560-ac85-8bcc5cb3338e"
      },
      "source": [
        "trainer = torch.optim.SGD(model.parameters(), lr=0.005)\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    train_iters, train_passed  = 0, 0\n",
        "    train_loss, train_acc = 0., 0.\n",
        "    \n",
        "    for X, y in train:\n",
        "        trainer.zero_grad()\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        l.backward()\n",
        "        trainer.step()\n",
        "        train_loss += l.item()\n",
        "        train_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        train_iters += 1\n",
        "        train_passed += len(X)\n",
        "    \n",
        "    test_iters, test_passed  = 0, 0\n",
        "    test_loss, test_acc = 0., 0.\n",
        "    for X, y in test:\n",
        "        y_pred = model(X)\n",
        "        l = loss(y_pred, y)\n",
        "        test_loss += l.item()\n",
        "        test_acc += (y_pred.argmax(dim=1) == y).sum().item()\n",
        "        test_iters += 1\n",
        "        test_passed += len(X)\n",
        "        \n",
        "    print(\"ep: {}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
        "        ep, train_loss / train_iters, train_acc / train_passed,\n",
        "        test_loss / test_iters, test_acc / test_passed)\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ep: 0, train_loss: 0.13144146234114118, train_acc: 0.9516, test_loss: 0.3539829196408391, test_acc: 0.8883\n",
            "ep: 1, train_loss: 0.11640336312195088, train_acc: 0.9577666666666667, test_loss: 0.3492190442979336, test_acc: 0.8892\n",
            "ep: 2, train_loss: 0.11053908781168309, train_acc: 0.9603166666666667, test_loss: 0.3478224392980337, test_acc: 0.8893\n",
            "ep: 3, train_loss: 0.10658491245926695, train_acc: 0.9620833333333333, test_loss: 0.34754922054708004, test_acc: 0.8895\n",
            "ep: 4, train_loss: 0.10352838994499217, train_acc: 0.9634333333333334, test_loss: 0.34770356547087433, test_acc: 0.89\n",
            "ep: 5, train_loss: 0.10101712988887696, train_acc: 0.9648333333333333, test_loss: 0.34823001762852074, test_acc: 0.8899\n",
            "ep: 6, train_loss: 0.09884543907927706, train_acc: 0.9658, test_loss: 0.348840266559273, test_acc: 0.8896\n",
            "ep: 7, train_loss: 0.0969310278905199, train_acc: 0.9665333333333334, test_loss: 0.3495632965117693, test_acc: 0.8896\n",
            "ep: 8, train_loss: 0.09521722446413751, train_acc: 0.9670166666666666, test_loss: 0.3503076288849115, test_acc: 0.8904\n",
            "ep: 9, train_loss: 0.09363773169511176, train_acc: 0.9677, test_loss: 0.3511854588985443, test_acc: 0.8909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG1qA5ahCqlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}